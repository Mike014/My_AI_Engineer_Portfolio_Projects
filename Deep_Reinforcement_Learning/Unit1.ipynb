{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Deep Reinforcement Learning**\n",
    "- **What is Deep Reinforcement Learning?**\n",
    "Deep Reinforcement Learning (Deep RL) is a **subset of Machine Learning** in which an **agent learns to behave in an environment** by **performing actions** and **observing the results**.\n",
    "\n",
    "### **What is Reinforcement Learning?**\n",
    "\n",
    "#### **The basic idea**\n",
    "**Reinforcement Learning (RL)** is based on learning through interaction with the environment. An **agent** (an AI) performs actions in the environment and receives **rewards** (positive or negative) as feedback.\n",
    "\n",
    "This process is similar to how humans and animals learn: through **trial and error**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Formal Definition of Reinforcement Learning**\n",
    "**Reinforcement Learning** is a framework for solving control (or decision) problems by building **agents** that learn from the environment. They do this by interacting with it through trial and error and receiving **rewards** (positive or negative) as their only feedback.\n",
    "\n",
    "### **The Reinforcement Learning Process**\n",
    "The **RL process** is a continuous cycle in which an **agent** interacts with the **environment** through a sequence of states, actions, and rewards.\n",
    "\n",
    "1. **The agent receives a state** \\( S_0 \\) from the environment (e.g. the first screen of a video game).\n",
    "2. **Based on this state, the agent performs an action** \\( A_0 \\) (e.g. move right).\n",
    "3. **The environment changes to a new state** \\( S_1 \\) (new screen).\n",
    "4. **The environment provides a reward** \\( R_1 \\) (e.g. +1 if the agent is not dead).\n",
    "\n",
    "This cycle repeats, forming a sequence of states, actions, and rewards.\n",
    "\n",
    "---\n",
    "\n",
    "### **The Agent's Goal: Maximizing Total Reward**\n",
    "The agent aims to **maximize the sum of cumulative rewards over time**, called **expected return**.\n",
    "\n",
    "**Reward Hypothesis**: In RL, every goal can be formulated as the **maximization of the expected cumulative reward**.\n",
    "To achieve the best behavior, the agent must learn to perform actions that **maximize the expected reward in the long run**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Markov Property and Markov Decision Processes (MDP)**\n",
    "The RL process is modeled as a **Markov Decision Process (MDP)**, where **only the current state** is sufficient to decide the next action.\n",
    "\n",
    "**Markov Property**:\n",
    "The agent does not need to remember **all the past history**, but only the **current state** to make the next decision.\n",
    "\n",
    "---\n",
    "\n",
    "### **Observations vs States**\n",
    "- **State (\\( S_t \\))**: represents a complete description of the environment (e.g. chess → we see the whole board).\n",
    "- **Observation (\\( O_t \\))**: is a partial description of the state (e.g. Super Mario → we see only a part of the level).\n",
    "\n",
    "**In our experiments, we will use \"state\" for both complete states and partial observations**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Action Space**\n",
    "The **action space** is the set of all possible actions.\n",
    "- **Discrete Space**: finite number of actions (e.g. Mario can only move in 4 directions: left, right, up, down).\n",
    "- **Continuous space**: infinite number of actions (e.g. a self-driving car can turn 20°, 21.1°, 21.2°, etc.).\n",
    "\n",
    "**This distinction is fundamental to choosing the most suitable RL algorithm**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Rewards and Discounting**\n",
    "The **reward** is the only feedback the agent receives to know if it has performed a correct action.\n",
    "\n",
    "The **cumulative reward** is the sum of all the rewards received over time:\n",
    "\n",
    "$$G_t = R_{t+1} + R_{t+2} + R_{t+3} + \\dots$$\n",
    "\n",
    "**The problem:** We cannot simply add up all future rewards.\n",
    "**Short-term rewards** are more predictable than long-term ones.\n",
    "\n",
    "For example, imagine a **mouse trying to eat cheese while avoiding a cat**:\n",
    "- The cheese close by is safe and easy to reach.\n",
    "- The cheese close to the cat is more dangerous.\n",
    "\n",
    "**Solution: Discounting with the factor $( \\gamma )$ (gamma)**\n",
    "\n",
    "We use a **discount factor** $( \\gamma )$ $(( 0 < \\gamma < 1 ))$, usually between **0.95 and 0.99**:\n",
    "- **If $( \\gamma )$ is large $(( \\sim 0.99 ))$** → the agent takes future rewards into account more.\n",
    "- **If $( \\gamma )$ is small $(( \\sim 0.9 ))$** → the agent focuses on immediate rewards.\n",
    "\n",
    "**Discounted reward formula:**\n",
    "\n",
    "$$G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots$$\n",
    "\n",
    "\n",
    "In practice, **the further away a reward is in time, the less it is considered \"certain\"**, and therefore it is discounted.\n",
    "\n",
    "---\n",
    "\n",
    "### **For DUMMIES**\n",
    "Reinforcement Learning is a **sequential decision** process, where an agent:\n",
    "1. Observes the **state** or **observation** of the environment.\n",
    "2. Performs an **action**.\n",
    "3. Receives a **reward** and a new state.\n",
    "4. Aims to **maximize the expected cumulative reward**, balancing between immediate and future rewards thanks to **discounting**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Types of Tasks in Reinforcement Learning**\n",
    "A task is an instance of a Reinforcement Learning problem. There are **two main types of tasks**:\n",
    "\n",
    "1. **Episodic Task**: has a **beginning and an end** (e.g. a level of a video game).\n",
    "2. **Continuing Task**: has **no end** (e.g. trading systems or industrial control)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **The Trade-Off between Exploration and Exploration**\n",
    "Before we explore methods for solving Reinforcement Learning problems, we need to understand a **key concept**: the **trade-off between exploration and exploitation**.\n",
    "\n",
    "- **Exploration:** The agent experiments with **random actions to discover** new information about the environment.\n",
    "\n",
    "- **Exploitation:** The agent uses the **information already known** to maximize the reward in the short term.\n",
    "\n",
    "### **Two Approaches to Solving Reinforcement Learning Problems**\n",
    "\n",
    "To build an effective RL agent, it is necessary to find the **optimal policy** $( \\pi^* )$, that is, the function that maximizes the expected reward. There are two main methods to learn this policy:\n",
    "\n",
    "1. **Policy-Based Methods (Direct Approach)**\n",
    "- The agent directly learns a policy function that associates each **state** $( s )$ with the **best action** $( a )$.\n",
    "- It can be **deterministic**, where the policy always returns the same action for each state:\n",
    "\n",
    "$$a = \\pi(s)$$\n",
    "\n",
    "- Or **stochastic**, where the policy defines a probability distribution over the actions:\n",
    "\n",
    "$$pi(a | s) = P(A = a | S = s)$$\n",
    "\n",
    "- Useful for complex environments with continuous actions.\n",
    "- Examples: Policy Gradient, PPO.\n",
    "\n",
    "2. **Value-Based Methods (Indirect Approach)**\n",
    "- The agent learns a value function that estimates how advantageous it is to be in a given state $( s )$.\n",
    "- The value function expresses the cumulative reward expected if the agent starts from state $( s )$ and follows the policy $( \\pi )$:\n",
    "\n",
    "$$V^\\pi(s) = \\mathbb{E} \\left[ \\sum_{t=0}^{\\infty} \\gamma^t R_{t+1} \\mid S_0 = s \\right]$$\n",
    "\n",
    "- The agent always chooses the action that takes it to the state with the highest value.\n",
    "- Useful for environments with discrete action spaces.\n",
    "- Examples: Q-Learning, Deep Q-Networks (DQN).\n",
    "\n",
    "In the next modules, we will explore the learning methods for optimizing these approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent builds a **memory** of the paths taken, recording the states crossed and the rewards obtained in response to the actions. This allows it to learn and improve over time, recovering past information to make more efficient decisions.\n",
    "\n",
    "### **How ​​does the agent use memory?**\n",
    "1. **Store the states visited** → Record each state $( S_t )$ and the action $( A_t )$ performed.\n",
    "2. **Record the reward obtained** → After each action, save the reward $( R_t )$.\n",
    "3. **Update the strategy** → If a state has led to a positive reward, the agent considers it more advantageous and will be more likely to repeat it in the future.\n",
    "4. **Retrieve similar states** → If the agent encounters a similar state again, it can use previous knowledge to choose the best possible action.\n",
    "\n",
    "This ability to **recover states and adapt** is what makes Reinforcement Learning effective, allowing the agent to learn increasingly optimized strategies without having to start from scratch each time.\n",
    "\n",
    "---\n",
    "\n",
    "### **Abstract**\n",
    "\n",
    "**Reinforcement Learning (RL)** is a computational method of action-based learning. An **agent** learns by interacting with the environment through **trial and error**, receiving **rewards** (positive or negative) as feedback.\n",
    "\n",
    "The **goal** of an RL agent is to **maximize the expected cumulative reward** (*expected return*), according to the **reward hypothesis**, which states that every goal can be described as the maximization of the expected cumulative reward.\n",
    "\n",
    "The **RL process** follows a continuous cycle:\n",
    "\n",
    "$$\\text{State} \\rightarrow \\text{Action} \\rightarrow \\text{Reward} \\rightarrow \\text{New State}$$\n",
    "\n",
    "To calculate the expected reward, a **discount factor** $(\\gamma)$ is used, which gives more weight to immediate rewards than to long-term ones, since they are more predictable.\n",
    "\n",
    "To solve an RL problem, you need to find the **optimal policy** $( \\pi^* )$, that is, the strategy that maximizes the expected return:\n",
    "1. **Policy-Based Methods** → The agent directly learns the policy function $( \\pi(s) )$, which assigns an action to each state.\n",
    "2. **Value-Based Methods** → The agent learns a value function $( V(s) )$, which estimates the expected return for each state and uses this function to define the policy.\n",
    "\n",
    "**Deep Reinforcement Learning (Deep RL)** is when **deep neural networks** are used to estimate the policy (policy-based) or the value of the states (value-based), hence the term *deep*."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
