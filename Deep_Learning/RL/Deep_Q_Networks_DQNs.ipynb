{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61937bd6",
   "metadata": {},
   "source": [
    "## **Deep Q-Networks (DQNs)**\n",
    "\n",
    "**DQNs** are an extension of **Q-Learning**, designed for environments with state spaces too large to use a traditional Q-Table. They use **deep neural networks** to approximate the function Q(s, a), i.e. the value of a given action in a given state.\n",
    "\n",
    "**Main advantages:**\n",
    "\n",
    "* They can work with **continuous or high-dimensional inputs**.\n",
    "* They avoid the combinatorial explosion problem of Q-tables.\n",
    "\n",
    "**Key components of DQNs:**\n",
    "\n",
    "1. **Main Q-Net**: a neural network that approximates the Q-function.\n",
    "2. **Target Network**: a less up-to-date copy of the main network, used to make training more stable.\n",
    "3. **Experience Replay**: a memory that preserves past experiences to train the network more efficiently and stably.\n",
    "\n",
    "**Main implementation phases:**\n",
    "\n",
    "1. **Initialization** of the environment (e.g. Gym) and parameters (learning rate, gamma, epsilonâ€¦).\n",
    "2. **Building of the Q network and target** in Keras (2 hidden layers of 24 neurons with ReLU activation).\n",
    "3. **Experience replay**: we memorize (state, action, reward, new state, done).\n",
    "4. **Training**: we update the weights using the Bellman Equation, with targets calculated by the target network.\n",
    "5. **Evaluation**: the agent acts in the environment with the learned policy and the total reward is calculated.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
