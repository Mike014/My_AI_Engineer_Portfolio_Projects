{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd805f19",
   "metadata": {},
   "source": [
    "# **Machine Learning Basics**\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486daac5",
   "metadata": {},
   "source": [
    "## **Introduction to Machine Learning**\n",
    "- **Deep learning** is a subset of machine learning.\n",
    "- To understand deep learning, mastering the basic principles of machine learning is essential.\n",
    "- This chapter provides an overview of fundamental concepts, with references to texts such as Murphy (2012) and Bishop (2006)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e26928a",
   "metadata": {},
   "source": [
    "### What is **\"learning\"** (E, T, P) and types of **tasks**\n",
    "\n",
    "***A machine \"learns\"*** if, given an *experience E*, it improves *performance P* on a *set of tasks T* (Mitchell's classic definition). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f2edd7",
   "metadata": {},
   "source": [
    "## **The Task** $T$\n",
    "ML tasks include:\n",
    "\n",
    "- **Classification**: Assign a label to an input.\n",
    "- **Regression**: Predict a numerical value.\n",
    "- **Transcription**: Convert unstructured data into text.\n",
    "- **Machine translation**: Convert sequences between languages.\n",
    "- **Structured output**: Produce data with complex relationships (e.g., parse trees).\n",
    "- **Anomaly detection**: Identify unusual events.\n",
    "- **Synthesis and sampling**: Generate new examples similar to the training data.\n",
    "- **Missing value imputation**: Predict absent values.\n",
    "- **Denoising**: Reconstruct clean data from corrupted versions.\n",
    "- **Density estimation**: Learn the probability distribution of the data.\n",
    "\n",
    "## **Performance Measures** $P$\n",
    "Examples: *accuracy*, *error rate*, *mean log-likelihood*.\n",
    "Performance is evaluated on a *test set* separate from the *training data*.\n",
    "\n",
    "## **Experience** $E$\n",
    "- **Supervised learning**: Labeled examples $(x, y)$.\n",
    "- **Unsupervised learning**: Data only $x$, no labels.\n",
    "- **Semi-supervised and reinforcement learning** are variants.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ce0ba7",
   "metadata": {},
   "source": [
    "## **Model capacity, overfitting/underfitting, and bias-variance trade-off**\n",
    "\n",
    "- **Capacity**: How well a model can represent complex functions; **too low capacity → underfitting**, **too high capacity → overfitting**.\n",
    "- **Bias-Variance trade-off**: As capacity increases, bias tends to decrease, variance tends to increase; generalization error often shows a U-shaped curve, with \"optimal\" capacity in between.\n",
    "- **Consistency**: It is desirable for the estimator to converge to the true value as the data grows (formally defined)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea8ff4b",
   "metadata": {},
   "source": [
    "## **Validation Set and Hyperparameters (Key Idea)**\n",
    "\n",
    "Many algorithms have **hyperparameters** (e.g., regularization strength, learning rate) that are **not learned from standard training and must be chosen with data outside of the training set** (validation set or cross-validation). This separates model training from hyperparameter selection, limiting leakage and conceptual overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d95950f",
   "metadata": {},
   "source": [
    "## **Estimators, Bias/Variance, and Maximum Likelihood (ML)**\n",
    "\n",
    "This chapter introduces **statistical terminology (estimators, bias, variance, consistency)**. Consistency requires, among other things, that the true data generator be within the model family and correspond to a single parameter value.\n",
    "\n",
    "### **Estimators**\n",
    "- Rule to guess parameters from data\n",
    "- **Bias**: Average error | **Variance**: Sensitivity to data\n",
    "- **Consistent** if: finds truth with infinite data AND true model in family\n",
    "\n",
    "### **Maximum Likelihood (ML)**\n",
    "- Choose parameters making data most probable\n",
    "- Maximize log-likelihood = Minimize NLL/Cross-entropy\n",
    "- **Linear regression example**: Assume Gaussian noise → ML = Minimize MSE\n",
    "\n",
    "### **ML Properties**\n",
    "- **With big data**: Consistent + Efficient (lowest variance)\n",
    "- **With small data**: Overfits (high variance)\n",
    "- **Solution**: Regularization (e.g., weight decay)\n",
    "  - Trade: Small bias increase for large variance reduction\n",
    "  - Better generalization\n",
    "\n",
    "- **Estimators** (Bias/Variance) → The \"thermometer\" that tells us whether the model is generalizing well\n",
    "- **Likelihood** → The \"engine\" that drives training toward probabilistically correct predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5208f332",
   "metadata": {},
   "source": [
    "## **Bayesian vs Frequentist Approaches in Machine Learning**\n",
    "\n",
    "### **Core Concepts**\n",
    "- **Prior**  $p(θ)$ : Initial beliefs about parameters *before* seeing data\n",
    "- **Posterior**  $p(θ|x_{1:m})$ : Updated beliefs *after* observing data (via Bayes' rule)\n",
    "- **Prediction**: Integrate over all possible parameters → **natural protection against overfitting**\n",
    "\n",
    "### **MAP (Maximum A Posteriori)**\n",
    "- Practical compromise: take the **mode of the posterior distribution**\n",
    "-  $θ_{MAP} = \\arg\\max[\\log p(x|θ) + \\log p(θ)] $\n",
    "- **Gaussian Prior** → L2 regularization (weight decay)\n",
    "- **Key insight**: Many classical regularizations = MAP with specific priors\n",
    "\n",
    "---\n",
    "\n",
    "### **PRACTICAL COMPARISON**\n",
    "\n",
    "| **Bayesian Approach** | **Frequentist (ML) Approach** |\n",
    "|----------------------|------------------------------|\n",
    "| ✅ **Better generalization** with limited data | ✅ **Simpler**, computationally efficient |\n",
    "| ✅ **Incorporates prior knowledge** | ✅ **Consistent & efficient** asymptotically |\n",
    "| ❌ **Computationally expensive** | ❌ **Tends to overfit** with small datasets |\n",
    "| ❌ **Prior choice can be subjective** | ❌ **Requires explicit regularization** |\n",
    "\n",
    "---\n",
    "\n",
    "### **MODERN PRACTICE**\n",
    "\n",
    "- **Large datasets**: Often prefer **ML + regularization** (equivalent to MAP)\n",
    "- **Limited data**: Full Bayesian approach can provide **better generalization**\n",
    "- **Modern Deep Learning**: Primarily **Frequentist** for efficiency, but with **heavy regularization** (dropout, batch norm, weight decay) that mimics Bayesian benefits\n",
    "\n",
    "**The bottom line**: Two perspectives on the same underlying mathematics, with choice depending on data size, computational resources, and need to incorporate existing domain knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405e9130",
   "metadata": {},
   "source": [
    "# **Supervised vs Unsupervised Learning: Deep Dive**\n",
    "\n",
    "## **SUPERVISED LEARNING**\n",
    "\n",
    "### **Core Concept:**\n",
    "We model **$p(y | x)$** - the probability of outputs given inputs\n",
    "\n",
    "### **Key Examples:**\n",
    "\n",
    "#### **1. Linear Regression (Gaussian)**\n",
    "$$p(y|x) = \\mathcal{N}(y; w^T x, \\sigma^2)$$\n",
    "$$\\min_w \\sum_{i=1}^m (y_i - w^T x_i)^2$$\n",
    "\n",
    "- **Output**: Continuous value\n",
    "- **Assumption**: Gaussian error\n",
    "- **Solution**: Closed-form (normal equations)\n",
    "\n",
    "#### **2. Logistic Regression (Binary Classification)**\n",
    "$$p(y=1|x) = \\sigma(w^T x) \\quad \\text{where} \\quad \\sigma(z) = \\frac{1}{1+e^{-z}}$$\n",
    "$$\\max_w \\sum_{i=1}^m \\left[y_i \\log(\\sigma(w^T x_i)) + (1-y_i)\\log(1-\\sigma(w^T x_i))\\right]$$\n",
    "\n",
    "- **Output**: Probability [0,1]\n",
    "- **No closed form** → numerical optimization (gradient descent)\n",
    "- **Loss function**: Negative Log-Likelihood (NLL)\n",
    "\n",
    "#### **3. Support Vector Machines (SVM)**\n",
    "$$\\min ||w||^2 \\quad \\text{subject to} \\quad y_i(w^T x_i + b) \\geq 1$$\n",
    "\n",
    "- **Output**: Class label (-1 or +1) **without probabilities**\n",
    "- **Kernel trick**: Implicit mapping to higher dimensions\n",
    "\n",
    "---\n",
    "\n",
    "## **UNSUPERVISED LEARNING**\n",
    "\n",
    "### **Core Concept:**\n",
    "We model the **intrinsic structure** of $x$ - without labels\n",
    "\n",
    "### **Main Examples:**\n",
    "\n",
    "#### **1. PCA (Principal Component Analysis)**\n",
    "$$\\max_w \\text{Var}(w^T x) = w^T \\Sigma w \\quad \\text{constraint:} \\quad ||w|| = 1$$\n",
    "$$\\min ||x - ww^T x||^2$$\n",
    "\n",
    "- **Interpretations**:\n",
    "  - **Geometric**: Coordinate axis rotation\n",
    "  - **Probabilistic**: Feature decorrelation\n",
    "  - **Compression**: Dimensionality reduction\n",
    "\n",
    "#### **2. k-Means Clustering**\n",
    "$$\\min \\sum_{j=1}^k \\sum_{x \\in C_j} ||x - \\mu_j||^2$$\n",
    "\n",
    "- **Representation**: One-hot encoding of cluster assignments\n",
    "- **Limitation**: Assumes spherical, similarly-sized clusters\n",
    "\n",
    "#### **3. Density Estimation**\n",
    "$$p_{\\text{model}}(x) \\approx p_{\\text{data}}(x)$$\n",
    "\n",
    "- **Applications**: Generative modeling, anomaly detection\n",
    "\n",
    "---\n",
    "\n",
    "## **UNIFIED RECIPE (Model + Cost + Optimizer)**\n",
    "\n",
    "### **Same Framework for Both:**\n",
    "\n",
    "| **Component** | **Supervised** | **Unsupervised** |\n",
    "|---------------|----------------|------------------|\n",
    "| **Model** | $p(y\\|x)$ | $p(x)$ or $f(x)$ |\n",
    "| **Cost** | NLL, MSE | Reconstruction, Likelihood |\n",
    "| **Optimizer** | Gradient Descent, Normal Equations | SVD, EM, Gradient Descent |\n",
    "\n",
    "### **Concrete Example - PCA as Optimization:**\n",
    "$$\\min_W ||X - XWW^T||^2_F \\quad \\text{subject to} \\quad W^T W = I$$\n",
    "Solution: $W^* = \\text{eigenvectors}(X^T X)[:, :k]$\n",
    "\n",
    "---\n",
    "\n",
    "## **FUNDAMENTAL DIFFERENCES**\n",
    "\n",
    "### **Supervised:**\n",
    "- **Goal**: Accurate prediction of $y$\n",
    "- **Data**: Labeled $(x_i, y_i)$\n",
    "- **Evaluation**: Accuracy, MSE, F1-score\n",
    "- **Risk**: Overfitting on labels\n",
    "\n",
    "### **Unsupervised:**\n",
    "- **Goal**: Discover latent structure\n",
    "- **Data**: Only $x_i$\n",
    "- **Evaluation**: Less objective (silhouette, likelihood)\n",
    "- **Risk**: Uninterpretable structures\n",
    "\n",
    "---\n",
    "\n",
    "## **IMPORTANT INTERCONNECTIONS**\n",
    "\n",
    "### **1. Supervised → Unsupervised**\n",
    "$$p(x,y) = p(y|x)p(x)$$\n",
    "We often only model $p(y|x)$ and ignore $p(x)$\n",
    "\n",
    "### **2. Unsupervised → Supervised**\n",
    "$$X_{\\text{original}} \\rightarrow \\text{PCA} \\rightarrow X_{\\text{reduced}} \\rightarrow \\text{Classifier}$$\n",
    "\n",
    "### **3. Semi-Supervised Learning**\n",
    "$$L_{\\text{total}} = \\alpha L_{\\text{supervised}} + (1-\\alpha) L_{\\text{unsupervised}}$$\n",
    "\n",
    "---\n",
    "\n",
    "## **PRACTICAL GUIDELINES**\n",
    "\n",
    "**Use Supervised when:**\n",
    "- You have labeled data\n",
    "- You want specific predictions\n",
    "- Input-output relationship is clear\n",
    "\n",
    "**Use Unsupervised when:**\n",
    "- Exploring unknown data\n",
    "- Finding hidden patterns\n",
    "- Preparing features for subsequent models\n",
    "\n",
    "**The beauty is that the same mathematical \"recipe\" works for both, demonstrating the fundamental unity of machine learning.**\n",
    "\n",
    "### **Real-world Applications:**\n",
    "\n",
    "**Supervised:**\n",
    "- Spam detection (classification)\n",
    "- House price prediction (regression)\n",
    "- Medical diagnosis\n",
    "\n",
    "**Unsupervised:**\n",
    "- Customer segmentation (clustering)\n",
    "- Anomaly detection in networks\n",
    "- Topic modeling in documents\n",
    "\n",
    "The choice depends entirely on your data availability and problem objectives!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6607e84",
   "metadata": {},
   "source": [
    "## THE UNIVERSAL MACHINE LEARNING RECIPE\n",
    "\n",
    "### THE 4 FUNDAMENTAL COMPONENTS:\n",
    "Every ML algorithm is built with:\n",
    "1. **DATASET** → $(X, y)$ (supervised) or $X$ (unsupervised)\n",
    "2. **MODEL** → $p_{\\text{model}}$ that describes the data\n",
    "3. **COST FUNCTION** → often Negative Log-Likelihood (NLL) ± regularization\n",
    "4. **OPTIMIZER** → closed-form **or** gradient descent/SGD\n",
    "\n",
    "**Why do some algorithms (decision trees, k-means) require special optimizers?**  \n",
    "Because they have cost functions with **flat regions** where gradient-based methods don't work!\n",
    "\n",
    "---\n",
    "\n",
    "## OPERATIONAL TAKEAWAYS - WHAT TO DO IN PRACTICE\n",
    "\n",
    "### 1. CLEARLY DEFINE THE PROBLEM\n",
    "- **Task (T)**: What should the model do?\n",
    "- **Performance (P)**: How do you measure success?\n",
    "- **Experience (E)**: What data does it learn from?\n",
    "- **METRIC** → must align with real objectives\n",
    "\n",
    "### 2. COMBAT OVERFITTING\n",
    "- **Regularization** (L2/L1, weight decay)\n",
    "- **Early stopping**\n",
    "- **More data** when possible\n",
    "- **BALANCE** the bias-variance trade-off\n",
    "\n",
    "### 3. CHOOSE THE ESTIMATION STRATEGY\n",
    "- **Default**: Maximum Likelihood (ML) via NLL/cross-entropy\n",
    "- **Limited data?** → Bayesian or MAP (incorporate prior knowledge)\n",
    "- **Large datasets?** → ML + regularization\n",
    "\n",
    "### 4. SCALE TRAINING\n",
    "- **SGD with minibatches** → essential for large datasets\n",
    "- **Modern variants** → Adam, RMSProp, etc.\n",
    "\n",
    "### 5. APPLY THE MENTAL RECIPE\n",
    "- Any ML problem = **Dataset + Model + Cost + Optimizer**\n",
    "- **Works for supervised AND unsupervised** → unified framework\n",
    "\n",
    "### 6. WATCH OUT FOR HIGH DIMENSIONS\n",
    "- **Curse of dimensionality** → exponentially harder problems\n",
    "- **SOLUTION**: Distributed representations and deep models\n",
    "- **Deep models learn hierarchical structures** that mitigate this problem\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78dca352",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
