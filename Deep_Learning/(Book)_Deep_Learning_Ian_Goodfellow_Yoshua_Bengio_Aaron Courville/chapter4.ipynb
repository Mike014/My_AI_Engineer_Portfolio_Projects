{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46fe53da",
   "metadata": {},
   "source": [
    "## Chapter 4 â€“ Numerical Computation with NumPy\n",
    "\n",
    "Machine learning algorithms often rely on heavy numerical computation, typically involving iterative methods rather than closed-form solutions. A major challenge is that real numbers must be represented using a finite number of bits, which introduces **rounding errors**, including **underflow** and **overflow**.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Underflow and Overflow\n",
    "\n",
    "* **Underflow** occurs when very small positive numbers are rounded to zero.\n",
    "* **Overflow** occurs when very large numbers are rounded to infinity.\n",
    "\n",
    "Example using NumPy:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Underflow example\n",
    "small_number = np.exp(-1000)\n",
    "print(\"Underflow example:\", small_number)  # prints 0.0\n",
    "\n",
    "# Overflow example\n",
    "large_number = np.exp(1000)\n",
    "print(\"Overflow example:\", large_number)  # prints inf\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 2. The Softmax Function and Numerical Instability\n",
    "\n",
    "The softmax function is defined as:\n",
    "\n",
    "$$\n",
    "\\text{softmax}(x)_i = \\frac{e^{x_i}}{\\sum_j e^{x_j}}\n",
    "$$\n",
    "\n",
    "Naive (unstable) implementation:\n",
    "\n",
    "```python\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "x = np.array([1000, 1000])\n",
    "print(\"Unstable softmax:\", softmax(x))  # Overflow, returns [nan nan]\n",
    "```\n",
    "\n",
    "**Problem**: This fails when `x` contains large values.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Numerically Stable Softmax\n",
    "\n",
    "To avoid overflow, subtract the max value:\n",
    "\n",
    "$$\n",
    "z = x - \\max(x)\n",
    "$$\n",
    "\n",
    "Stable implementation:\n",
    "\n",
    "```python\n",
    "def stable_softmax(x):\n",
    "    z = x - np.max(x)\n",
    "    exp_z = np.exp(z)\n",
    "    return exp_z / np.sum(exp_z)\n",
    "\n",
    "x = np.array([1000, 1000])\n",
    "print(\"Stable softmax:\", stable_softmax(x))  # returns [0.5 0.5]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Log-Softmax (Stable Implementation)\n",
    "\n",
    "The log-softmax is defined as:\n",
    "\n",
    "$$\n",
    "\\text{log\\_softmax}(x)_i = x_i - \\log\\left(\\sum_j e^{x_j}\\right)\n",
    "$$\n",
    "\n",
    "Stable implementation in NumPy:\n",
    "\n",
    "```python\n",
    "def stable_log_softmax(x):\n",
    "    z = x - np.max(x)\n",
    "    log_sum_exp = np.log(np.sum(np.exp(z)))\n",
    "    return z - log_sum_exp\n",
    "\n",
    "x = np.array([1000, 1000])\n",
    "print(\"Stable log-softmax:\", stable_log_softmax(x))  # returns [-0.693147, -0.693147]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Final Notes\n",
    "\n",
    "* Always use numerically stable versions of functions like softmax and log-softmax.\n",
    "* NumPy allows you to write these versions easily and avoid pitfalls like overflow/underflow, ensuring robust and accurate computations.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
