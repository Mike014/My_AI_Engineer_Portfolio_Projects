{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0487fca",
   "metadata": {},
   "source": [
    "# Softmax \n",
    "\n",
    "* **Purpose:** The softmax function generalizes logistic regression to the **multi-class** case (more than two categories).\n",
    "\n",
    "* **Class scores (logits):** For each class (i), a score (logit) is computed as\n",
    "  [\n",
    "  z_i = w_i^\\top x + b_i\n",
    "  ]\n",
    "  In 1D, these correspond to different lines; in higher dimensions, they are **dot products** between the input vector (x) and the parameter vectors (w_i).\n",
    "\n",
    "* **Decision (hard prediction):** The predicted class is the index with the highest score:\n",
    "  [\n",
    "  \\hat{y} = \\arg\\max_i z_i\n",
    "  ]\n",
    "  (In the examples: blue/red/green regions in 1D, or in 2D, points assigned to the parameter vector (w_i) producing the largest dot product.)\n",
    "\n",
    "* **Probabilities (soft prediction):** The logits are transformed into probabilities using the softmax function:\n",
    "  [\n",
    "  p_i = \\frac{e^{z_i}}{\\sum_j e^{z_j}}\n",
    "  ]\n",
    "  Each class gets a probability between 0 and 1, and all probabilities sum to 1.\n",
    "\n",
    "* **Geometric intuition (2D / MNIST example):**\n",
    "\n",
    "  * Each **class** is represented by a parameter vector (w_i).\n",
    "  * Each sample (x) is classified according to the (w_i) with the highest dot product (the “closest” vector).\n",
    "  * The MNIST example shows how 28×28 grayscale images (784 pixels) are flattened into vectors for classification; for visualization, the idea is simplified to 2D.\n",
    "\n",
    "* **Training:** In practice, softmax is trained using **cross-entropy loss**, which encourages the correct class to have the highest logit.\n",
    "\n",
    "* **Summary:**\n",
    "\n",
    "  1. Compute logits (z_i) for each class.\n",
    "  2. Use **argmax** to pick the predicted class.\n",
    "  3. Use **softmax** to convert logits into class probabilities.\n",
    "  4. Train the model with **cross-entropy loss** for multi-class classification.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
