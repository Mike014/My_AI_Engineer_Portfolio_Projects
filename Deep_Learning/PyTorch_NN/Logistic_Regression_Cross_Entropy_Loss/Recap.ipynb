{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53c972d6",
   "metadata": {},
   "source": [
    "# Why MSE Is Not Good for Classification\n",
    "\n",
    "* With a **threshold function** (0/1), the cost as a function of the parameters has **large flat regions** — the gradient is 0, so optimization stops even if there are still misclassified samples.\n",
    "* Even when using a **sigmoid** with **MSE**, the cost surface can still be **uninformative** (flat in key areas), especially in higher-dimensional spaces (W, B). The result: convergence depends on initialization and is often slow or unstable.\n",
    "\n",
    "---\n",
    "\n",
    "# From MLE to Cross-Entropy (CE)\n",
    "\n",
    "1. **Model**: for logistic regression\n",
    "   $(p_\\theta(y=1\\mid x)=\\sigma(w^\\top x+b))$\n",
    "   $(p_\\theta(y=0\\mid x)=1-\\sigma(\\cdot))$\n",
    "2. **Likelihood** on a dataset $({(x_n,y_n)})$:\n",
    "   $(\\mathcal{L}(\\theta)=\\prod_n p_\\theta(y_n\\mid x_n))$\n",
    "3. **Log-likelihood**:\n",
    "   $(\\log\\mathcal{L}=\\sum_n [y_n\\log \\hat{p}_n + (1-y_n)\\log(1-\\hat{p}_n)])$\n",
    "4. **From maximization to minimization**: minimize the **negative log-likelihood** → **Binary Cross-Entropy**\n",
    "   $[\n",
    "   \\mathcal{J}(\\theta)= -\\frac{1}{N}\\sum_n [y_n\\log \\hat{p}_n + (1-y_n)\\log(1-\\hat{p}_n)]\n",
    "   ]$\n",
    "   This loss is **smooth**, with **useful gradients** almost everywhere and well-behaved minima.\n",
    "\n",
    "---\n",
    "\n",
    "# Geometric Intuition\n",
    "\n",
    "* With threshold / MSE: cost changes in “steps” → flat regions with no gradient.\n",
    "* With **sigmoid + CE**: the curve/surface is **smooth**; shifting the boundary slightly changes the cost → the optimizer “feels” which way to move.\n",
    "\n",
    "---\n",
    "\n",
    "# PyTorch Implementation (Binary)\n",
    "\n",
    "* **Model** (1D → 1D) and **loss** options:\n",
    "\n",
    "  * **Option A (recommended)** – output **logits** (no sigmoid in `forward`) + `nn.BCEWithLogitsLoss()`\n",
    "  * **Option B** – output includes **sigmoid** + `nn.BCELoss()` (less numerically stable)\n",
    "\n",
    "* **Optimizer**: typically SGD or Adam\n",
    "\n",
    "* **Training loop**: `forward → loss → loss.backward() → optimizer.step() → optimizer.zero_grad()`\n",
    "  After training, apply a **0.5 threshold** on (\\hat{p}) to obtain class 0/1.\n",
    "\n",
    "---\n",
    "\n",
    "### Minimal Example (recommended: BCEWithLogitsLoss)\n",
    "\n",
    "```python\n",
    "import torch, torch.nn as nn\n",
    "\n",
    "X = torch.randn(200, 1)                       # 1D feature\n",
    "y = (X + 0.5 * torch.randn_like(X) > 0).float().view(-1, 1)  # labels 0/1\n",
    "\n",
    "class Logistic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(1, 1)\n",
    "    def forward(self, x):\n",
    "        return self.lin(x)  # output logits\n",
    "\n",
    "model = Logistic()\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "opt = torch.optim.SGD(model.parameters(), lr=1e-2)\n",
    "\n",
    "for epoch in range(100):\n",
    "    logits = model(X)\n",
    "    loss = loss_fn(logits, y)\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "with torch.no_grad():\n",
    "    probs = torch.sigmoid(model(X))\n",
    "    y_hat = (probs >= 0.5).float()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# Practical Notes\n",
    "\n",
    "* **Multi-class**: use `nn.CrossEntropyLoss()` with **logits** (no softmax in `forward`), and integer targets (\\in{0,\\dots,K-1}).\n",
    "* **Numerical stability**: prefer `BCEWithLogitsLoss` (it combines sigmoid and BCE safely).\n",
    "* **Metrics**: track **loss + accuracy** (or F1/AUROC) to see if the model is truly reducing false positives/negatives.\n",
    "\n",
    "---\n",
    "\n",
    "**In short:**\n",
    "Cross-Entropy naturally arises from **Maximum Likelihood Estimation** for probabilistic classifiers.\n",
    "Compared to MSE / threshold losses, it produces a **smooth cost surface with meaningful gradients**, making gradient-based optimization **more efficient and reliable**.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
