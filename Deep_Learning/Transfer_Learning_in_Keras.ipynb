{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Transfer Learning in Keras**\n",
    "\n",
    "### **Transfer Learning Concept**\n",
    "\n",
    "- **Transfer Learning** is a **technique** that allows you to **reuse pre-trained models** on large datasets for **new and related tasks**.\n",
    "- It **works like the human learning process**: those who know how to play the piano can learn the organ more easily thanks to shared principles.\n",
    "- It is **especially useful when limited data is available**, reducing training time and improving performance.\n",
    "\n",
    "#### **How does that work?**\n",
    "- A **pre-trained model** is used (e.g. VGG16 on ImageNet).\n",
    "- You **reuse the convolutional basis** of the model to **extract features**.\n",
    "- New **fully connected layers** are **added to adapt it** to the new activity.\n",
    "- It is **decided whether to freeze the weights** of the **pre-trained model** or to **fine-tunate some layers**.\n",
    "\n",
    "#### **Advantages**\n",
    "- **Reduced training time**: The model starts with features that have already been learned.\n",
    "- **Improved accuracy**: The pre-trained model has already extracted relevant patterns.\n",
    "- **Requires less data**: Thanks to features learned from larger datasets.\n",
    "- **Lower resource consumption**: You avoid having to train a complex network from scratch.\n",
    "\n",
    "##### **Implementation in Keras**\n",
    "**Main steps**\n",
    "#### *Import the necessary forms*:\n",
    "\n",
    "  * VGG16 from tensorflow.keras.applications\n",
    "  * Sequential, Dense, Flatten from tensorflow.keras.models\n",
    "  * ImageDataGenerator for image preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\dell\\anaconda3\\envs\\my_env\\lib\\site-packages (2.18.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement nyumpy (from versions: none)\n",
      "ERROR: No matching distribution found for nyumpy\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade tensorflow nyumpy Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary forms\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.applications import VGG16\n",
    "# from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   emotion                                             pixels     Usage\n",
      "0        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...  Training\n",
      "1        0  151 150 147 155 148 133 111 140 170 174 182 15...  Training\n",
      "2        2  231 212 156 164 174 138 161 173 182 200 106 38...  Training\n",
      "3        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...  Training\n",
      "4        6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...  Training\n"
     ]
    }
   ],
   "source": [
    "# Path of the dataset\n",
    "csv_path = r\"C:\\Users\\DELL\\Desktop\\AI_Engineer\\Deep_Learning\\archive\\fer2013.csv\"\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (28709, 48, 48, 1), Test set: (7178, 48, 48, 1)\n"
     ]
    }
   ],
   "source": [
    "# Extract image labels and data\n",
    "labels = df[\"emotion\"].values\n",
    "pixels = df[\"pixels\"].values\n",
    "\n",
    "# Convert pixels from string to number array\n",
    "X = np.array([np.fromstring(pixel, sep=\" \") for pixel in pixels], dtype=\"float32\")\n",
    "\n",
    "# Normalize pixels (0-255 to 0-1)\n",
    "X /= 255.0\n",
    "\n",
    "# Resize images to 48x48x1 (grayscale)\n",
    "X = X.reshape(-1, 48, 48, 1)\n",
    "\n",
    "# Convert labels to one-hot categories\n",
    "y = to_categorical(labels, num_classes=7) # FER2013 has 7 emotion classes\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check the dimensions\n",
    "print(f\"Training set: {X_train.shape}, Test set: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert images to RGB (3 channels) and resize to 224x224\n",
    "X_train_rgb = np.repeat(X_train, 3, axis=-1)  # From 1 channel to 3\n",
    "X_test_rgb = np.repeat(X_test, 3, axis=-1)\n",
    "\n",
    "X_train_resized = np.array([cv2.resize(img, (224, 224)) for img in X_train_rgb])\n",
    "X_test_resized = np.array([cv2.resize(img, (224, 224)) for img in X_test_rgb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the VGG16 model\n",
    "\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# weights='imagenet' → Uses pre-trained weights.\n",
    "# include_top=False → Excludes the original output layers.\n",
    "# input_shape=(224, 224, 3) → Input size (RGB, 224x224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze the weights of the pre-trained model\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# This prevents the model weights from being changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new layers for classification\n",
    "\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    Flatten(),\n",
    "    Dense(256, activation=\"relu\"),\n",
    "    Dense(7, activation=\"softmax\")  \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=['accuracy'])\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model \n",
    "model.fit(X_train_resized, y_train, epochs=10, batch_size=32, validation_data=(X_test_resized, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Using Pretrained Models as Feature Extractors in Keras**\n",
    "\n",
    "#### **What is a pre-trained model?**\n",
    "**Pre-trained models are neural networks that have already been trained on large datasets**, such as ImageNet, to learn useful features that can be reused for new tasks.\n",
    "\n",
    "Instead of completely retraining them (fine-tuning), **you can use them directly as feature pullers, without changing their weights**.\n",
    "\n",
    "#### **Use as Feature Extractors** \n",
    "A pre-trained model can be used to **extract features from new data** and apply them to different downstream tasks such as: \n",
    "- **Clustering** (grouping similar data), \n",
    "- **Visualization** (analyzing and representing features), \n",
    "- **Dimensionality reduction** (for simpler models).\n",
    "\n",
    "##### **Fine-Tuning: Optimization of Upper Levels**\n",
    "\n",
    "**Fine-Tuning unlocks** some of the **top layers of the frozen model** and retrains them on the new dataset.\n",
    "This helps the model better adapt to the new data, improving performance compared to using the feature extractor alone.\n",
    "Fine-Tuning = Transfer Learning → A pre-trained model is adapted to a new related task, leveraging features learned from a larger dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
