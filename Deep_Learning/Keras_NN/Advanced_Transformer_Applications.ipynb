{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Advanced Transformer Applications**\n",
    "\n",
    "**Transformers** have evolved beyond NLP, finding application in fields such as **computer vision, speech recognition, and reinforcement learning** due to their flexible self-attention architecture.\n",
    "\n",
    "- **Vision Transformers (ViTs)** demonstrate that multi-head attention can replace CNNs in image analysis. By dividing an image into *patches*, the model treats them as sequences of data, allowing for a more global and adaptable representation.\n",
    "\n",
    "- **In speech recognition**, Transformers process audio into spectrograms, capturing long-term dependencies in speech signals. Models such as **Wav2Vec** and **Speech Transformer** outperform traditional approaches by combining convolutional and Transformer layers to improve transcription.\n",
    "\n",
    "- **In reinforcement learning**, Transformer models such as **Decision Transformer** leverage their ability to model sequences to predict actions based on past trajectories, improving learning efficiency.\n",
    "\n",
    "This versatility makes Transformers essential tools for tackling complex problems beyond language processing, opening up new possibilities in areas such as visual perception, audio processing, and autonomous artificial intelligence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Abstract: Transformers for Time Series Forecasting**\n",
    "\n",
    "**Transformers** are revolutionizing **time series** forecasting, offering significant advantages over traditional models such as **ARIMA**, **RNN**, and **LSTM**.\n",
    "\n",
    "### **Why use Transformers for Time Series?**\n",
    "- **Captures long-term dependencies** better than RNN/LSTM.\n",
    "- **Parallel processing**, accelerating training.\n",
    "- **Handles variable-length sequences** and missing data with greater flexibility.\n",
    "\n",
    "### **Model Architecture**\n",
    "A Transformer model for time series follows this structure:\n",
    "1. **Embedding Layer** → Converts sequences into dense vectors.\n",
    "2. **Transformer Blocks** → Layers with **self-attention** and **feed-forward networks** to analyze relationships between data over time.\n",
    "3. **Final Dense Layer** → Predicts the next value in the series.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer, Dense, LayerNormalization, Dropout, MultiHeadAttention, Input, Embedding, Flatten, LSTM\n",
    "from tensorflow.keras.models import Sequential, Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Transformer Block\n",
    "class TransformerBlock(Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = Sequential([\n",
    "            Dense(ff_dim, activation=\"relu\"),\n",
    "            Dense(embed_dim)\n",
    "        ])\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)  # Corrected the second dropout layer\n",
    "\n",
    "    def call(self, inputs, training, mask=None):\n",
    "        attn_output = self.att(inputs, inputs, inputs, attention_mask=mask)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)  # Corrected the LayerNormalization call\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)  # Corrected the second dropout layer\n",
    "        return self.layernorm2(out1 + ffn_output)  # Corrected the LayerNormalization call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data Preparation**\n",
    "- A **stock price** dataset is used.\n",
    "- The data is **normalized** with **MinMaxScaler**.\n",
    "- **Input sequences** are created with the previous values ​​and labeled with the next value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\dell\\anaconda3\\envs\\ai_env\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\dell\\anaconda3\\envs\\ai_env\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.22.4 in c:\\users\\dell\\anaconda3\\envs\\ai_env\\lib\\site-packages (from pandas) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\dell\\anaconda3\\envs\\ai_env\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\dell\\anaconda3\\envs\\ai_env\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\dell\\anaconda3\\envs\\ai_env\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\dell\\anaconda3\\envs\\ai_env\\lib\\site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\dell\\anaconda3\\envs\\ai_env\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\dell\\anaconda3\\envs\\ai_env\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\dell\\anaconda3\\envs\\ai_env\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[100.99342831],\n",
       "       [ 99.77349641],\n",
       "       [101.3954271 ],\n",
       "       ...,\n",
       "       [198.13620067],\n",
       "       [199.62384106],\n",
       "       [198.51019471]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv(\"stock_prices.csv\")\n",
    "data = data[[\"Close\"]].values\n",
    "\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the Data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "data = scaler.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data for training\n",
    "def create_dataset(data, time_step=1):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(data) - time_step - 1):\n",
    "        a = data[i:(i + time_step), 0] # Slice a sequence from a data\n",
    "        X.append(a)\n",
    "        Y.append(data[i + time_step, 0])\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "time_step = 60\n",
    "X, Y = create_dataset(data, time_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Data [[0.03884152]\n",
      " [0.02747753]\n",
      " [0.04258624]\n",
      " ...\n",
      " [0.94375214]\n",
      " [0.95760991]\n",
      " [0.947236  ]]\n",
      "Length of X [[0.03884152 0.02747753 0.04258624 ... 0.05038847 0.06278687 0.07525617]\n",
      " [0.02747753 0.04258624 0.05936031 ... 0.06278687 0.07525617 0.04861998]\n",
      " [0.04258624 0.05936031 0.02708906 ... 0.07525617 0.04861998 0.05455432]\n",
      " ...\n",
      " [0.93648347 0.94093553 0.95291693 ... 0.97151402 0.96533161 0.97918741]\n",
      " [0.94093553 0.95291693 0.93497963 ... 0.96533161 0.97918741 0.95922183]\n",
      " [0.95291693 0.93497963 0.93935929 ... 0.97918741 0.95922183 0.94375214]]\n",
      "Shape of first Element in X: (60,)\n",
      "Shape of X after reshape (1939, 60, 1)\n",
      "Shape of X (1939, 60, 1)\n",
      "Shape of Y (1939,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of Data {data}\")\n",
    "print(f\"Length of X {X}\")\n",
    "print(f\"Shape of first Element in X:\", X[0].shape if len(X) > 0 else \"X is empty\")\n",
    "\n",
    "if len(X) > 0:\n",
    "    X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "    print(f\"Shape of X after reshape {X.shape}\")\n",
    "\n",
    "print(f\"Shape of X {X.shape}\")\n",
    "print(f\"Shape of Y {Y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_8\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_8\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_block_7             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">396,032</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_block_8             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">396,032</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_block_9             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">396,032</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_block_10            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">396,032</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7680</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">7,681</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_8 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m1\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_18 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │           \u001b[38;5;34m256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_block_7             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │       \u001b[38;5;34m396,032\u001b[0m │\n",
       "│ (\u001b[38;5;33mTransformerBlock\u001b[0m)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_block_8             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │       \u001b[38;5;34m396,032\u001b[0m │\n",
       "│ (\u001b[38;5;33mTransformerBlock\u001b[0m)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_block_9             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │       \u001b[38;5;34m396,032\u001b[0m │\n",
       "│ (\u001b[38;5;33mTransformerBlock\u001b[0m)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_block_10            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │       \u001b[38;5;34m396,032\u001b[0m │\n",
       "│ (\u001b[38;5;33mTransformerBlock\u001b[0m)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7680\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_27 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │         \u001b[38;5;34m7,681\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,592,065</span> (6.07 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,592,065\u001b[0m (6.07 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,592,065</span> (6.07 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,592,065\u001b[0m (6.07 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the Transformer Model\n",
    "input_shape = (X.shape[1], X.shape[2])\n",
    "inputs = Input(shape=input_shape)\n",
    "\n",
    "# Embedding layer\n",
    "x = Dense(128)(inputs)\n",
    "\n",
    "# Transformer Block\n",
    "for _ in range(4):\n",
    "    x = TransformerBlock(embed_dim=128, num_heads=4, ff_dim=512)(x, training=True)\n",
    "\n",
    "# Output Layer\n",
    "x = Flatten()(x)\n",
    "outputs = Dense(1)(x)\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs, outputs)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This table represents the **Transformer model architecture** for time series forecasting, generated with the Keras `.summary()` method. Let's analyze row by row the layers and parameters involved.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. General model structure**\n",
    "- The model accepts inputs with shape **(None, 60, 1)** (60 time steps, 1 feature).\n",
    "- After several Transformer layers, the output is transformed into a single value (the prediction of the next value in the time series).\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Layers detail**\n",
    "\n",
    "| **Layer (type)** | **Output Shape** | **Parameters (#)** | **Description** |\n",
    "|---------------------------|--------------------|--------------------|------------------|\n",
    "| **input_layer_8 (InputLayer)** | (None, 60, 1) | 0 | Input layer with temporal sequences of length 60 and 1 feature. |\n",
    "| **dense_18 (Dense)** | (None, 60, 128) | 256 | Projects the input into a 128-dimensional space (embedding). |\n",
    "| **transformer_block_7** (TransformerBlock) | (None, 60, 128) | 396,032 | First Transformer block with self-attention. |\n",
    "| **transformer_block_8** (TransformerBlock) | (None, 60, 128) | 396,032 | Second Transformer block, continues processing the sequence. |\n",
    "| **transformer_block_9** (TransformerBlock) | (None, 60, 128) | 396,032 | Third Transformer block, allows the model to learn deeper relationships. |\n",
    "| **transformer_block_10** (TransformerBlock) | (None, 60, 128) | 396,032 | Fourth and final Transformer block, captures long-term dependencies. |\n",
    "| **flatten (Flatten)** | (None, 7680) | 0 | Transforms the sequence into a one-dimensional vector for the output layer. |\n",
    "| **dense_27 (Dense)** | (None, 1) | 7,681 | Final layer that generates the time series forecast. |\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Number of parameters analysis**\n",
    "- **Layer Dense (Embedding)**: It has **256 parameters**, because it transforms the input into a richer representation (input_dim=1, output_dim=128).\n",
    "- **Each Transformer Block**: Contains **396,032 parameters**, because it includes:\n",
    "- **Self-Attention** (multiple heads),\n",
    "- **Feed Forward Network** (dimensionality expansion),\n",
    "- **Norm & Dropout Layer**.\n",
    "- **Flatten**: Has no parameters, transforms the data without learning anything.\n",
    "- **Last Dense**: Has **7,681 parameters**, because it connects **7680 inputs** to **1 output**.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **Model Training**\n",
    "- **Optimizer:** Adam\n",
    "- **Loss Function:** Mean Squared Error (MSE)\n",
    "- The model is trained on the prepared data to learn the time trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the Model\n",
    "model.compile(optimizer=\"Adam\", loss=\"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 109ms/step - loss: 7.3802\n",
      "Epoch 2/20\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 112ms/step - loss: 0.2023\n",
      "Epoch 3/20\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 109ms/step - loss: 0.1404\n",
      "Epoch 4/20\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 109ms/step - loss: 0.2066\n",
      "Epoch 5/20\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 108ms/step - loss: 0.0734\n",
      "Epoch 6/20\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 109ms/step - loss: 0.0882\n",
      "Epoch 7/20\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 108ms/step - loss: 0.0654\n",
      "Epoch 8/20\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 110ms/step - loss: 0.0342\n",
      "Epoch 9/20\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 111ms/step - loss: 0.0265\n",
      "Epoch 10/20\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 110ms/step - loss: 0.0221\n",
      "Epoch 11/20\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 109ms/step - loss: 0.0214\n",
      "Epoch 12/20\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 117ms/step - loss: 0.0133\n",
      "Epoch 13/20\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 139ms/step - loss: 0.0093\n",
      "Epoch 14/20\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 114ms/step - loss: 0.0089\n",
      "Epoch 15/20\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 122ms/step - loss: 0.0111\n",
      "Epoch 16/20\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 128ms/step - loss: 0.0071\n",
      "Epoch 17/20\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 122ms/step - loss: 0.0062\n",
      "Epoch 18/20\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 110ms/step - loss: 0.0068\n",
      "Epoch 19/20\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 111ms/step - loss: 0.0065\n",
      "Epoch 20/20\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 109ms/step - loss: 0.0071\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1c6bb9b6260>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the Model\n",
    "model.fit(X, Y, epochs=20, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **Evaluation and Forecasting**\n",
    "- After training, the model is tested on new sequences.\n",
    "- The predicted values ​​are **denormalized** to bring them back to the original scale.\n",
    "- The results are displayed graphically to compare the predictions with the real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\anaconda3\\envs\\ai_env\\lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Define the model \n",
    "model = Sequential()\n",
    "model.add(LSTM(50, return_sequences=True, input_shape=(time_step, 1)))\n",
    "# return_sequences=True: Indicates that the LSTM layer should return the entire output sequence for each input.\n",
    "model.add(LSTM(50, return_sequences=False))\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer=\"adam\", loss=\"mean_squared_error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 77ms/step - loss: 0.0842\n",
      "Epoch 2/10\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 346ms/step - loss: 4.3960e-04\n",
      "Epoch 3/10\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 99ms/step - loss: 3.5800e-04\n",
      "Epoch 4/10\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 88ms/step - loss: 3.5796e-04\n",
      "Epoch 5/10\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 68ms/step - loss: 3.9438e-04\n",
      "Epoch 6/10\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 60ms/step - loss: 3.8814e-04\n",
      "Epoch 7/10\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 54ms/step - loss: 3.9883e-04\n",
      "Epoch 8/10\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 49ms/step - loss: 3.8917e-04\n",
      "Epoch 9/10\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 45ms/step - loss: 3.8923e-04\n",
      "Epoch 10/10\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 42ms/step - loss: 3.8278e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1c6c1fddc00>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(X, Y, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "predictions = model.predict(X)\n",
    "\n",
    "# Inverse transform the predictions to get the original scale\n",
    "predictions = scaler.inverse_transform(predictions)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(scaler.inverse_transform(data), label=\"True Data\")\n",
    "plt.plot(np.arange(time_step, time_step + len(predictions)), predictions, label=\"Predictions\")\n",
    "\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Stock Prices\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
