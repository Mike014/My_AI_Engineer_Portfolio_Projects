{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Introduction to Transformers in Keras**\n",
    "\n",
    "- **Transformers** have **revolutionized Natural Language Processing (NLP)** and are also being applied in other fields such as image processing and time series forecasting.\n",
    "- The model was introduced in the paper **\"[Attention is All You Need\" by Vaswani et al](https://neuron-ai.at/attention-is-all-you-need/#:~:text=In%20the%20paper%20%E2%80%9CAttention%20Is%20All%20You%20Need%E2%80%9D%2C,attention%20mechanism%20without%20using%20sequence-aligned%20RNNs%20or%20convolution.)**.\n",
    "- **Difference from RNNs**: Transformers use Self-Attention, allowing parallel processing of data, making them more efficient than traditional sequential models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Transformers Architecture**\n",
    "Transformers are **divided into two main components**:\n",
    "\n",
    "- **Encoder**\n",
    "- **Decoder**\n",
    "\n",
    "- Both **contain**:\n",
    "\n",
    "  - **Self-Attention Mechanism**: Allows the **model to weight words** in the **context of a sentence**.\n",
    "  - **Feed-Forward Neural Networks (FFNN)**: **Transforms** the data after the attention mechanism.\n",
    "\n",
    "- **Self-Attention**\n",
    "Each **word** is **represented by three vectors**:\n",
    "1. *Query* (Q), Represent **what a word \"asks\" to other words** in the sentence.\n",
    "2. *Key* (K), Represent **what each word \"offers\"** to other words.\n",
    "3. *Value* (V), Represent the **actual information that each word \"provides\"**.\n",
    "\n",
    "The **attention score is calculated as a dot product between Query and Key**, **normalized** via **Softmax**.\n",
    "This mechanism allows to **capture dependencies** even between **distant words in the sequence**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Implementation in Keras**\n",
    "\n",
    "1. **_Self-Attention Layer_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "class SelfAttention(Layer):\n",
    "    def __init__(self, embed_dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.query_dense = tf.keras.layers.Dense(embed_dim)\n",
    "        self.key_dense = tf.keras.layers.Dense(embed_dim)\n",
    "        self.value_dense = tf.keras.layers.Dense(embed_dim)\n",
    "        self.softmax = tf.keras.layers.Softmax(axis=-1)\n",
    "        # embed_dim: Size of embeddings.\n",
    "        # self.query_dense, self.key_dense, self.value_dense: Dense layers to compute query, key, and value matrices.\n",
    "        # self.softmax: Softmax layer to normalize attention scores.\n",
    "\n",
    "    def call(self, inputs):\n",
    "        query = self.query_dense(inputs)\n",
    "        key = self.key_dense(inputs)\n",
    "        value = self.value_dense(inputs)\n",
    "\n",
    "        attention_scores = tf.matmul(query, key, transpose_b=True) / tf.sqrt(float(self.embed_dim))\n",
    "        attention_weights = self.softmax(attention_scores)\n",
    "        output = tf.matmul(attention_weights, value)\n",
    "        return output\n",
    "    #    query, key, value: Computes query, key, and value matrices from inputs.\n",
    "    #    attention_scores: Computes attention scores as the dot product of query and key, normalized by the square root of the embedding size.\n",
    "    #    attention_weights: Softmaxes the attention scores to get attention weights.\n",
    "    #    output: Computes the output as a weighted average of the values ​​using the attention weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Encoders in Transformers**\n",
    "- An **encoder consists of multiple layers of self-attention** and **feed-forward networks**.\n",
    "- It **includes residual connections** and **layer normalization to stabilize training**.\n",
    "- **Positional encoding** is **used to maintain word order**.\n",
    "\n",
    "2. **_Implementing a Transformer Encoder_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.attention = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        # This layer implements multi-head attention, which allows the model to focus on different parts of the input sequence simultaneously\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(ff_dim, activation=\"relu\"),\n",
    "            tf.keras.layers.Dense(embed_dim)\n",
    "        ])\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6) # A small value to avoid division by zero.\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        # Normalization levels to stabilize and accelerate training\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        # Dropout levels to prevent overfitting\n",
    "\n",
    "    def call(self, inputs, training): \n",
    "        attn_output = self.attention(inputs, inputs) # Calculate the output of multi-head attention.\n",
    "        attn_output = self.dropout1(attn_output, training=training) # Apply dropout to attention output\n",
    "        out1 = self.layernorm1(inputs + attn_output) # Adds the original input to the attention output (residual connection) and applies normalization.\n",
    "\n",
    "        ffn_output = self.ffn(out1) # Pass the normalized output through the feed-forward network.\n",
    "        ffn_output = self.dropout2(ffn_output, training=training) # Apply dropout to the output of the feed-forward network.\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "    \n",
    "# Transformer uses multi-head attention to capture dependencies between words in the input sequence and \n",
    "# a feed-forward network to transform the attention output. \n",
    "# Residual connections and layer normalization help stabilize and improve model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Decoder in Transformers**\n",
    "- The **decoder is similar to the encoder**, but **includes a cross-attention mechanism to connect to the encoder output**.\n",
    "- It **generates sequences based on the context** provided by the encoder.\n",
    "\n",
    "3. **_Implementing a Transformer Decoder_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.attention1 = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.attention2 = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(ff_dim, activation=\"relu\"),\n",
    "            tf.keras.layers.Dense(embed_dim)\n",
    "        ])\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "        # embed_dim: Size of embeddings.\n",
    "        # num_heads: Number of attention heads.\n",
    "        # ff_dim: Size of feed-forward network.\n",
    "        # rate: Dropout rate.\n",
    "        # self.attention1: Multi-head attention level for decoder input.\n",
    "        # self.attention2: Multi-head attention level for encoder output.\n",
    "        # self.ffn: Feed-forward network.\n",
    "        # self.layernorm1, self.layernorm2, self.layernorm3: Normalization levels.\n",
    "        # self.dropout1, self.dropout2, self.dropout3: Dropout levels.\n",
    "\n",
    "    def call(self, inputs, encoder_output, training):\n",
    "        attn1 = self.attention1(inputs, inputs)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn1)\n",
    "\n",
    "        attn2 = self.attention2(out1, encoder_output)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(out1 + attn2)\n",
    "\n",
    "        ffn_output = self.ffn(out2)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        return self.layernorm3(out2 + ffn_output)\n",
    "        # attn1: Compute multi-head attention on decoder input.\n",
    "        # attn1 = self.dropout1(attn1, training=training): Apply dropout to attention output.\n",
    "        # out1 = self.layernorm1(inputs + attn1): Add original input to attention output (residual connection) and apply normalization.\n",
    "        # attn2: Compute multi-head attention on encoder output.\n",
    "        # attn2 = self.dropout2(attn2, training=training): Apply dropout to attention output.\n",
    "        # out2 = self.layernorm2(out1 + attn2): Add previous attention output to encoder output (residual connection) and apply normalization.\n",
    "        # ffn_output: Pass through feed-forward network.\n",
    "        # ffn_output = self.dropout3(ffn_output, training=training): Apply dropout to the output of the feed-forward network.\n",
    "        # return self.layernorm3(out2 + ffn_output): Add the output of the feed-forward network to the previous output (residual connection) and apply the final normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Transformers** have surpassed **RNNs** due to their **ability to process inputs in parallel**.\n",
    "- Their **self-attention mechanism is the heart of the model**, allowing for better understanding of context.\n",
    "- They are **used in NLP**, **vision**, **time-series forecasting**, and many other applications.\n",
    "- Their architecture is **based on encoders and decoders**, with self-attention mechanisms and feed-forward networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformers (For Dummies) **look** similar to RNNs because they process text sequences, but their architecture is **fundamentally different**. Let me explain the flow schematically:\n",
    "\n",
    "1. **Tokenized Input**:\n",
    "- Text is converted into tokens (numbers representing words or subwords).\n",
    "- Tokens are transformed into **dense vectors** with **Word Embeddings** (e.g. Word2Vec, GloVe, or directly learned by the model).\n",
    "\n",
    "2. **Positional Encoding**:\n",
    "- Since Transformers **do not use time like RNNs**, a mechanism is needed to maintain the order of tokens.\n",
    "- A **positional encoding** is added to the vectors to make the model understand the sequence.\n",
    "\n",
    "3. **Encoder** (Repeated Blocks):\n",
    "- **Self-Attention**: Each word compares itself to all other words in the sentence, calculating how important it is compared to the others (attention weights).\n",
    "- **Feed-Forward Neural Network (FFNN)**: A neural network transforms the processed vector.\n",
    "- **Layer Norm + Residual Connections**: Stabilize the training.\n",
    "\n",
    "4. **Decoder** (Blocks similar to the encoder but with the addition of Cross-Attention):\n",
    "- **Masked Self-Attention**: Similar to Self-Attention, but prevents looking at future tokens (avoids cheating in text generation).\n",
    "- **Cross-Attention**: Allows the decoder to \"look\" at the encoder output.\n",
    "- **FFNN + Residuals & Normalization**.\n",
    "\n",
    "5. **Final Output**:\n",
    "- The decoder produces a probability distribution over each word in the vocabulary.\n",
    "- The most likely token is selected to form the answer.\n",
    "\n",
    "---\n",
    "\n",
    "### **Main Differences between RNN and Transformers**\n",
    "| Feature | RNN (Recurrent Neural Network) | Transformer |\n",
    "|---------|--------------------------------|-----------------------------------|\n",
    "| **Processing** | Sequential (one token at a time) | Parallel (all tokens simultaneously) |\n",
    "| **Context Handling** | Limited short-term memory, prone to \"forgetting\" | Global self-attention, captures long-range dependencies |\n",
    "| **Architecture** | Recurrent structure, state depends on previous inputs | Attention-based, all tokens interact directly |\n",
    "| **Training Speed** | Slow, difficult to parallelize effectively | Fast, highly parallelizable |\n",
    "| **Long-Range Dependencies** | Struggles to capture information from distant tokens | Excels at capturing long-range relationships |\n",
    "| **Gradient Flow** | Prone to vanishing/exploding gradients in long sequences | More stable gradient flow due to attention mechanism |\n",
    "| **Computational Complexity** | Linear with sequence length | Quadratic with sequence length (in self-attention) |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Integrate EWC (Elastic Weight Consolidation)** with **Self-Attention** to create a **mechanism that maintains memory over time**, making the **model less likely to forget previous information**. \n",
    "\n",
    "1. **Basic Concept**\n",
    "- **Self-Attention** allows the **model to weight words** based on the **context of a sentence**.\n",
    "- **EWC protects the critical weights** for previous tasks, **using the Fisher matrix to estimate the importance of each weight**.\n",
    "- If we **combine EWC with Self-Attention**, we can **stabilize the connections between keywords**, making the **model \"remember\" important relationships between words over time**.\n",
    "\n",
    "If **Self-Attention distributes weights dynamically** and **EWC stabilizes some of these weights**, how can we balance **learning new information** without limiting the model's adaptability too much?\n",
    "How much should we **protect memory** and how much **should we allow the model to adapt**?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
