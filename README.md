# Overview 

This repository presents some Machine Learning concepts acquired from my **AI Engineering Professional Certificate specialization course**.

## Create a virtual environment with **Conda**:
 ```bash
 conda create -n ml_env python=3.12
 conda activate ml_env
```

### Glossary of Terms Used in the Course

1. **Algorithm**: A set of step-by-step instructions for solving a problem or making a prediction in machine learning.
2. **Bias**: Systematic error introduced by the model when it fails to capture the complexity of the data, leading to underfitting.
3. **Churn Prediction**: The process of predicting whether a customer will abandon a service or subscription.
4. **Classes**: The possible outcomes or output categories predicted by the model.
5. **Classification**: Classification predicts which category a piece of data belongs to by assigning it a discrete label.
6. **Classification with KNN**: Method that assigns the class based on the majority of the K nearest neighbors.
7. **Distance Euclidean**: Metric for calculating the distance between two points in multidimensional space.
8. **Distance Manhattan**: Alternative to Euclidean distance, based on orthogonal (grid-like) paths.
9. **Epsilon-Tube**: Margin around the prediction in SVR, where points within the margin are not penalized.
10. **Feature Irrelevant**: Useless or redundant variables that increase noise in the model and reduce accuracy.
11. **Feature Relevant**: Input variables that help the model improve prediction.
12. **Feature Standardization**: Process to make features comparable, reducing their unbalanced impact on predictions.
13. **Features**: The input (independent) variables that describe the observations.
14. **Gamma**: Parameter of RBF and polynomial kernels that controls how much a single data point influences the separation.
15. **Hard Margin**: Requires a perfect separation between classes, with a rigid margin.
16. **Hyperplane**: Multidimensional surface that separates data into two classes.
17. **Inference**: Refers to the process of using a trained model to make predictions on new data. It is where the model, based on patterns learned during training, provides output for inputs that have never been seen before.
18. **K-Nearest Neighbors (KNN)**: Supervised algorithm that uses the nearest neighbors to classify or make predictions.
19. **Kernel**: Function that transforms data to make it separable in high-dimensional spaces.
20. **Labeled Data**: Input data that includes the correct answer (label or class) for training.
21. **Linear Kernel**: Uses a simple hyperplane to separate classes.
22. **Margin**: Distance between the hyperplane and the nearest data points (support vectors).
23. **MSE**: Measures how close the target values at a node are to the mean.
24. **Overfitting**: When the model learns specific details and noise from the training data, but fails to generalize to new data (K too low).
25. **Parameter C**: Controls the trade-off between a stricter or softer separation.
26. **Polynomial Kernel**: Maps data into a more complex space with polynomial functions.
27. **Ponderazione dei vicini**: Technique to assign greater weight to closer neighbors in classification.
28. **Regression**: Regression is a statistical technique that estimates a relationship between a continuous dependent variable and one or more independent variables.
29. **Regression with KNN**: Method that assigns a numerical value by taking the mean or median of the values of the K nearest neighbors.
30. **RBF (Radial Basis Function) Kernel**: Uses a transformation based on the distance between points to separate complex data.
31. **Soft Margin**: Allows some misclassifications to improve the model's generalization.
32. **Squilibrio nelle classi**: When some classes are much more present than others in the dataset, influencing predictions.
33. **Standardizzazione delle feature**: Process to make features comparable, reducing their unbalanced impact on predictions.
34. **Supervised Learning**: A learning method in which the model is trained on labeled data (with known features and targets).
35. **Support Vector Machines (SVM)**: Supervised machine learning algorithm for classification and regression.
36. **Support Vector Regression (SVR)**: Variant of SVM for regression, which predicts continuous values.
37. **Support Vectors**: The data points closest to the hyperplane, which influence the separation of classes.
38. **Target**: The dependent or output variable that the model is intended to predict.
39. **Underfitting**: Occurs when a machine learning model is too simplistic to capture the underlying patterns in the data (K too high).
40. **Unsupervised Learning**: A learning method in which the model works on unlabeled data, looking for patterns or structures in the data.
41. **Valore di K**: Number of neighbors considered to determine the class or target value of a new point.

