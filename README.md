# Overview 

This repository presents some Machine Learning concepts acquired from my **AI Engineering Professional Certificate specialization course**.

## Create a virtual environment with **Conda**:
 ```bash
 conda create -n ml_env python=3.12
 conda activate ml_env
```

### Glossary of Terms Used in the Course

1. **Algorithm**: A set of step-by-step instructions for solving a problem or making a prediction in machine learning.
2. **Bias**: Systematic error introduced by the model when it fails to capture the complexity of the data, leading to underfitting.
3. **Churn Prediction**: The process of predicting whether a customer will abandon a service or subscription.
4. **Classes**: The possible outcomes or output categories predicted by the model.
5. **Classification**: Classification predicts which category a piece of data belongs to by assigning it a discrete label.
6. **Epsilon-Tube**: Margin around the prediction in SVR, where points within the margin are not penalized.
7. **Features**: The input (independent) variables that describe the observations.
8. **Gamma**: Parameter of RBF and polynomial kernels that controls how much a single data point influences the separation.
9. **Hard Margin**: Requires a perfect separation between classes, with a rigid margin.
10. **Inference**: Refers to the process of using a trained model to make predictions on new data. It is where the model, based on patterns learned during training, provides output for inputs that have never been seen before.
11. **Hyperplane**: Multidimensional surface that separates data into two classes.
12. **Kernel**: Function that transforms data to make it separable in high-dimensional spaces.
13. **Linear Kernel**: Uses a simple hyperplane to separate classes.
14. **Labeled Data**: Input data that includes the correct answer (label or class) for training.
15. **Margin**: Distance between the hyperplane and the nearest data points (support vectors).
16. **MSE**: Measures how close the target values at a node are to the mean.
17. **Overfitting**: Where the model learns specific details and noise from the training data, but fails to generalize to new data.
18. **Parameter C**: Controls the trade-off between a stricter or softer separation.
19. **Polynomial Kernel**: Maps data into a more complex space with polynomial functions.
20. **RBF (Radial Basis Function) Kernel**: Uses a transformation based on the distance between points to separate complex data.
21. **Regression**: Regression is a statistical technique that estimates a relationship between a continuous dependent variable and one or more independent variables.
22. **Sigmoid Kernel**: Function similar to the sigmoid used in logistic regression.
23. **Soft Margin**: Allows some misclassifications to improve the model's generalization.
24. **Supervised Learning**: A learning method in which the model is trained on labeled data (with known features and targets).
25. **Support Vector Machines (SVM)**: Supervised machine learning algorithm for classification and regression.
26. **Support Vector Regression (SVR)**: Variant of SVM for regression, which predicts continuous values.
27. **Support Vectors**: The data points closest to the hyperplane, which influence the separation of classes.
28. **Target**: The dependent or output variable that the model is intended to predict.
29. **Underfitting**: Occurs when a machine learning model is too simplistic to capture the underlying patterns in the data.
30. **Unsupervised Learning**: A learning method in which the model works on unlabeled data, looking for patterns or structures in the data.

